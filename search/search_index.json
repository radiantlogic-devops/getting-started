{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Radiant Logic DevOps This site provides resources for DevOps professionals to deploy the RadiantOne platform. Docker images and Kubernetes for orchestration allows for easily deploying the RadiantOne platform in the cloud. Benefits Streamline deployments - deploy RadiantOne in a consistent fashion with the least manual configuration required. Also, reduce the amount of hardware and/or virtual machines required to deploy the solution. Simplify patches and upgrades using rolling updates. Easily monitor multi-node clusters and ensure they are resilient to failure. Increase fault tolerance - self-healing nodes ensures high-availability and less downtime. License A valid license is required to use RadiantOne. For Docker and Kubernetes deployments, have your Radiant Logic license key ready. If you need a license key, contact your Radiant Logic Account Representative.","title":"Home"},{"location":"#welcome-to-radiant-logic-devops","text":"This site provides resources for DevOps professionals to deploy the RadiantOne platform. Docker images and Kubernetes for orchestration allows for easily deploying the RadiantOne platform in the cloud.","title":"Welcome to Radiant Logic DevOps"},{"location":"#benefits","text":"Streamline deployments - deploy RadiantOne in a consistent fashion with the least manual configuration required. Also, reduce the amount of hardware and/or virtual machines required to deploy the solution. Simplify patches and upgrades using rolling updates. Easily monitor multi-node clusters and ensure they are resilient to failure. Increase fault tolerance - self-healing nodes ensures high-availability and less downtime.","title":"Benefits"},{"location":"#license","text":"A valid license is required to use RadiantOne. For Docker and Kubernetes deployments, have your Radiant Logic license key ready. If you need a license key, contact your Radiant Logic Account Representative.","title":"License"},{"location":"configmapyaml/","text":"Overview Sections in the configmap.yaml file to customize: Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed. ZooKeeper Service Your ZooKeeper Service as described in the ZooKeeper yaml file. ZK_CONN_STR: \"myzk:2181\" RadiantOne Cluster Name Your RadantOne cluster name: ZK_CLUSTER: \"fid\" Credentials to connect to ZooKeeper The ZooKeeper client credentials, for RadiantOne nodes to connect to ZooKeeper. ZK_PASSWORD: RadiantOne Super User Credentials Directory Super User credentials. FID_PASSWORD: In the default configmap.yaml, the ZooKeeper client credentials and the RadiantOne FID super user (e.g. cn=directory manager) credentials are set to the same value of: secret1234 kind: Secret metadata: name: rootcreds type: Opaque data: username: Y249RGlyZWN0b3J5IE1hbmFnZXI= password: c2VjcmV0MTIzNA== The encoded values can be generated using this command (example below is for the password): echo -n \"secret1234\" | base64 c2VjcmV0MTIzNA== You can leverage Kubernetes secrets instead of using clear text passwords for the ZooKeeper client credentials and the Directory Super User credentials in the .yaml file. The password key can be used to map to both the ZK_PASSWORD and FID_PASSWORD. To use different passwords for the ZooKeeper client credentials and the RadiantOne FID super user password, add another key (e.g. fid_password) and add it in the secrets config. echo -n \"mysecret1234\" | base64 bXlzZWNyZXQxMjM0 kind: Secret metadata: name: rootcreds type: Opaque data: username: password: fid_password: bXlzZWNyZXQxMjM0 You also need to refer to that secret key in the fid.yaml file. - name: FID_PASSWORD valueFrom: secretKeyRef: name: rootcreds key: fid_password RadiantOne License Your RadiantOne FID license key value: LICENSE: \"{rlib}INSERT_YOUR_RLI_LICENSE_KEY\"","title":"Overview"},{"location":"configmapyaml/#overview","text":"Sections in the configmap.yaml file to customize: Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed.","title":"Overview"},{"location":"configmapyaml/#zookeeper-service","text":"Your ZooKeeper Service as described in the ZooKeeper yaml file. ZK_CONN_STR: \"myzk:2181\"","title":"ZooKeeper Service"},{"location":"configmapyaml/#radiantone-cluster-name","text":"Your RadantOne cluster name: ZK_CLUSTER: \"fid\"","title":"RadiantOne Cluster Name"},{"location":"configmapyaml/#credentials-to-connect-to-zookeeper","text":"The ZooKeeper client credentials, for RadiantOne nodes to connect to ZooKeeper. ZK_PASSWORD:","title":"Credentials to connect to ZooKeeper"},{"location":"configmapyaml/#radiantone-super-user-credentials","text":"Directory Super User credentials. FID_PASSWORD: In the default configmap.yaml, the ZooKeeper client credentials and the RadiantOne FID super user (e.g. cn=directory manager) credentials are set to the same value of: secret1234 kind: Secret metadata: name: rootcreds type: Opaque data: username: Y249RGlyZWN0b3J5IE1hbmFnZXI= password: c2VjcmV0MTIzNA== The encoded values can be generated using this command (example below is for the password): echo -n \"secret1234\" | base64 c2VjcmV0MTIzNA== You can leverage Kubernetes secrets instead of using clear text passwords for the ZooKeeper client credentials and the Directory Super User credentials in the .yaml file. The password key can be used to map to both the ZK_PASSWORD and FID_PASSWORD. To use different passwords for the ZooKeeper client credentials and the RadiantOne FID super user password, add another key (e.g. fid_password) and add it in the secrets config. echo -n \"mysecret1234\" | base64 bXlzZWNyZXQxMjM0 kind: Secret metadata: name: rootcreds type: Opaque data: username: password: fid_password: bXlzZWNyZXQxMjM0 You also need to refer to that secret key in the fid.yaml file. - name: FID_PASSWORD valueFrom: secretKeyRef: name: rootcreds key: fid_password","title":"RadiantOne Super User Credentials"},{"location":"configmapyaml/#radiantone-license","text":"Your RadiantOne FID license key value: LICENSE: \"{rlib}INSERT_YOUR_RLI_LICENSE_KEY\"","title":"RadiantOne License"},{"location":"docker/","text":"Docker Radiant Logic provides preconfigured images of RadiantOne in Docker containers. Once deployed, the product is entirely operable. The containers are preconfigured to interoperate with other containers in the stack. Docker Compose Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration. Compose works in all environments: production, staging, development, testing, as well as CI workflows. Samples can be found in Radiant Logic's Git Hub Site . These samples provide a starting point for how to integrate different services using a Compose file and to manage their deployment with Docker Compose. Note: The following samples are intended for use in local development environments such as project setups, tinkering with software stacks, etc. These samples are not supported for production environments. Getting started These instructions guide you through the bootstrap phase of creating and deploying samples of containerized RadiantOne deployments with Docker Compose. Prerequisites Install Docker and Docker Compose. Windows or macOS: Install Docker Desktop Linux: Install Docker and then Docker Compose Download samples from the Radiant Logic Git Hub . Running a sample The root directory of each sample contains the docker-compose.yaml which describes the configuration of service components. All samples can be run in a local environment by going into the root directory of each one and executing: docker-compose up -d Check the README.md of each sample to get more details on the structure and what is the expected output. To stop and remove all containers of the sample application run: docker-compose down","title":"Docker"},{"location":"docker/#docker","text":"Radiant Logic provides preconfigured images of RadiantOne in Docker containers. Once deployed, the product is entirely operable. The containers are preconfigured to interoperate with other containers in the stack.","title":"Docker"},{"location":"docker/#docker-compose","text":"Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application\u2019s services. Then, with a single command, you create and start all the services from your configuration. Compose works in all environments: production, staging, development, testing, as well as CI workflows. Samples can be found in Radiant Logic's Git Hub Site . These samples provide a starting point for how to integrate different services using a Compose file and to manage their deployment with Docker Compose. Note: The following samples are intended for use in local development environments such as project setups, tinkering with software stacks, etc. These samples are not supported for production environments.","title":"Docker Compose"},{"location":"docker/#getting-started","text":"These instructions guide you through the bootstrap phase of creating and deploying samples of containerized RadiantOne deployments with Docker Compose.","title":"Getting started"},{"location":"docker/#prerequisites","text":"Install Docker and Docker Compose. Windows or macOS: Install Docker Desktop Linux: Install Docker and then Docker Compose Download samples from the Radiant Logic Git Hub .","title":"Prerequisites"},{"location":"docker/#running-a-sample","text":"The root directory of each sample contains the docker-compose.yaml which describes the configuration of service components. All samples can be run in a local environment by going into the root directory of each one and executing: docker-compose up -d Check the README.md of each sample to get more details on the structure and what is the expected output. To stop and remove all containers of the sample application run: docker-compose down","title":"Running a sample"},{"location":"fidyaml/","text":"Overview The following sections in the RadiantOne FID yaml file can be customized. Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed. Ports If you don\u2019t want to use the default ports, customize the following sections in the .yaml file: spec: ports: - port: 9100 name: admin-http - port: 2389 name: ldap - port: 2636 name: ldap2 ports: - containerPort: 2181 name: zk-client - containerPort: 7070 name: cp-http - containerPort: 7171 name: cp-https - containerPort: 9100 name: admin-http - containerPort: 9101 name: admin-https - containerPort: 2389 name: ldap - containerPort: 2636 name: ldaps - containerPort: 8089 name: http - containerPort: 8090 name: https ZooKeeper Client Credentials To configure the credentials that RadiantOne uses to connect to ZooKeeper, set the ZK_PASSWORD property. - name: ZK_PASSWORD valueFrom: secretKeyRef: name: key: You can leverage Kubernetes secrets instead of using clear text passwords for the ZooKeeper client credentials and the Directory Super User credentials in the .yaml file. RadiantOne FID Super User Credentials To configure the credentials used for the Radiantone Super User (e.g. cn=directory manager), set the FID_PASSWORD property. - name: FID_PASSWORD valueFrom: secretKeyRef: name: key:","title":"Overview"},{"location":"fidyaml/#overview","text":"The following sections in the RadiantOne FID yaml file can be customized. Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed.","title":"Overview"},{"location":"fidyaml/#ports","text":"If you don\u2019t want to use the default ports, customize the following sections in the .yaml file: spec: ports: - port: 9100 name: admin-http - port: 2389 name: ldap - port: 2636 name: ldap2 ports: - containerPort: 2181 name: zk-client - containerPort: 7070 name: cp-http - containerPort: 7171 name: cp-https - containerPort: 9100 name: admin-http - containerPort: 9101 name: admin-https - containerPort: 2389 name: ldap - containerPort: 2636 name: ldaps - containerPort: 8089 name: http - containerPort: 8090 name: https","title":"Ports"},{"location":"fidyaml/#zookeeper-client-credentials","text":"To configure the credentials that RadiantOne uses to connect to ZooKeeper, set the ZK_PASSWORD property. - name: ZK_PASSWORD valueFrom: secretKeyRef: name: key: You can leverage Kubernetes secrets instead of using clear text passwords for the ZooKeeper client credentials and the Directory Super User credentials in the .yaml file.","title":"ZooKeeper Client Credentials"},{"location":"fidyaml/#radiantone-fid-super-user-credentials","text":"To configure the credentials used for the Radiantone Super User (e.g. cn=directory manager), set the FID_PASSWORD property. - name: FID_PASSWORD valueFrom: secretKeyRef: name: key:","title":"RadiantOne FID Super User Credentials"},{"location":"helm/","text":"Helm Helm charts for FID and Zookeeper deployment Helm must be installed to use the charts. Please refer to Helm's documentation to get started. TL;DR $ helm repo add radiantone https://radiantlogic-devops.github.io/helm $ helm install my-release radiantone/fid Add Helm repo Once Helm has been set up correctly, add the repo as follows: helm repo add radiantone https://radiantlogic-devops.github.io/helm If you had already added this repo earlier, run helm repo update to retrieve the latest versions of the packages. You can then run helm search repo radiantone to see the charts. Remove Helm repo helm repo remove radiantone Install Zookeeper Prerequisites Kubernetes 1.18+ Helm 3 Charts Install Zookeeper Install Zookeeper with default values helm install --namespace=<name space> <release name> radiantone/zookeeper Install Zookeeper with overridden values helm install --namespace=<name space> <release name> radiantone/zookeeper \\ --set replicaCount=\"5\" List Zookeeper releases helm list --namespace=<name space> Upgrade a Zookeeper release helm upgrade --namespace=<name space> <release name> radiantone/zookeeper Delete a Zookeeper release helm delete --namespace=<name space> <release name> Install FID Prerequisites Kubernetes 1.18+ Helm 3 Charts Install FID Install FID with default values helm install --namespace=<name space> <release name> radiantone/fid Install FID with overridden values helm install --namespace=<name space> <release name> radiantone/fid \\ --set zk.connectionString=\"zk.dev:2181\" \\ --set zk.ruok=\"http://zk.dev:8080/commands/ruok\" \\ --set fid.license=\"<FID cluster license>\" \\ --set fid.rootPassword=\"test1234\" Note: Curly brackets in the liense must be escaped --set fid.license=\"\\{rlib\\}xxx\" List FID releases helm list --namespace=<name space> Upgrade FID release helm upgrade --namespace=<name space> <release name> radiantone/fid --set image.tag=7.3.17 Delete FID release helm delete --namespace=<name space> <release name>","title":"Helm"},{"location":"helm/#helm","text":"Helm charts for FID and Zookeeper deployment Helm must be installed to use the charts. Please refer to Helm's documentation to get started.","title":"Helm"},{"location":"helm/#tldr","text":"$ helm repo add radiantone https://radiantlogic-devops.github.io/helm $ helm install my-release radiantone/fid","title":"TL;DR"},{"location":"helm/#add-helm-repo","text":"Once Helm has been set up correctly, add the repo as follows: helm repo add radiantone https://radiantlogic-devops.github.io/helm If you had already added this repo earlier, run helm repo update to retrieve the latest versions of the packages. You can then run helm search repo radiantone to see the charts.","title":"Add Helm repo"},{"location":"helm/#remove-helm-repo","text":"helm repo remove radiantone","title":"Remove Helm repo"},{"location":"helm/#install-zookeeper","text":"","title":"Install Zookeeper"},{"location":"helm/#prerequisites","text":"Kubernetes 1.18+ Helm 3","title":"Prerequisites"},{"location":"helm/#charts","text":"","title":"Charts"},{"location":"helm/#install-zookeeper_1","text":"Install Zookeeper with default values helm install --namespace=<name space> <release name> radiantone/zookeeper Install Zookeeper with overridden values helm install --namespace=<name space> <release name> radiantone/zookeeper \\ --set replicaCount=\"5\" List Zookeeper releases helm list --namespace=<name space> Upgrade a Zookeeper release helm upgrade --namespace=<name space> <release name> radiantone/zookeeper Delete a Zookeeper release helm delete --namespace=<name space> <release name>","title":"Install Zookeeper"},{"location":"helm/#install-fid","text":"","title":"Install FID"},{"location":"helm/#prerequisites_1","text":"Kubernetes 1.18+ Helm 3","title":"Prerequisites"},{"location":"helm/#charts_1","text":"","title":"Charts"},{"location":"helm/#install-fid_1","text":"Install FID with default values helm install --namespace=<name space> <release name> radiantone/fid Install FID with overridden values helm install --namespace=<name space> <release name> radiantone/fid \\ --set zk.connectionString=\"zk.dev:2181\" \\ --set zk.ruok=\"http://zk.dev:8080/commands/ruok\" \\ --set fid.license=\"<FID cluster license>\" \\ --set fid.rootPassword=\"test1234\" Note: Curly brackets in the liense must be escaped --set fid.license=\"\\{rlib\\}xxx\" List FID releases helm list --namespace=<name space> Upgrade FID release helm upgrade --namespace=<name space> <release name> radiantone/fid --set image.tag=7.3.17 Delete FID release helm delete --namespace=<name space> <release name>","title":"Install FID"},{"location":"kubernetes/","text":"Kubernetes You can use Kubernetes to orchestrate the configuration and deployment of RadiantOne. Radiant Logic provides DevOps images for deployments on cloud platforms such as Amazon Web Services (AWS) using Amazon Elastic Kubernetes Service (EKS) and Microsoft Azure Kubernetes Service (AKS). Deployment Prerequisites Before deploying RadiantOne, you should already be familiar with Kubernetes Pods, Services and StatefulSets. See the Kubernetes Documentation for details. You should also be familiar with Kubernetes kubectl commands. See the Kubernetes Cheat Sheet for details. In addition, you should: Have a supported Kubernetes cluster running in the cloud. A commonly deployed Kubernetes cluster is Amazon Elastic Kubernetes Service . For a highly available architecture, the underlying Kubernetes cluster should support at least two pods running RadiantOne nodes and three pods running ZooKeeper. Note - Kubernetes v1.18+ is required. The pods running RadiantOne nodes need at least 2 CPUs and 4 GiB memory. The pods running ZK need at least 2 CPUs and 2 GiB memory. Install and configure the Kubernetes kubectl command-line tool on the machine where you will manage the Kubernetes cluster from. This utility controls the Kubernetes Cluster. An example is AWS CLI . Have a RadiantOne configuration exported from a Linux Dev/QA environment. After installing RadiantOne in the Kubernetes cluster, you can import the configuration and make any needed configuration updates from the RadiantOne Main Control Panel or from command line using the vdsconfig utility. Manifest Files Download the StatefulSet manifests: configmap.yaml, fid-aws.yaml and zk-aws.yaml, on the machine where you will manage the Kubernetes cluster from. Contact Radiant Logic at support@radiantlogic.com for access to these files. Zookeeper The StatefulSet manifest, zk-aws.yaml, creates a ZooKeeper cluster that consists of three Pods by default. Although the Kubernetes web console can be used to create new stateful sets, the steps below leverage the kubectl command line tool. Perform the following steps on the machine where you have downloaded the kubectl command line utilty and saved the yaml files. Installation Update zk-aws.yaml file if necessary. You can modify the ZooKeeper ports, number of nodes (ZOOKEEPER_FLEET_SIZE) in the ensemble (3 is the default, but you can make it 5 if needed), name of the Kubernetes service linked to the ZooKeeper nodes (make sure this matches what is defined in the configmap.yaml). Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed. Open a command prompt and navigate to the location where you have installed the kubectl command line tool. Create the ZooKeeper StatefulSet from the zk.yaml. Indicate the path to the ZooKeeper yaml file. In the example shown below, it is located in the same location at the kubectl tool. kubectl --namespace=prod apply -f zk-aws.yaml Note \u2013 if no namespace is used in the Kubernetes cluster, you can omit the --namespace property. Validating the ZooKeeper Service Before installing RadiantOne nodes, make sure the ZooKeeper service is running. There should be one leader node and two follower nodes (if you kept the default fleet size of 3). The steps in this section describe how to do this from the Kubernetes cluster web dashboard. From the Kubernetes web dashboard, navigate to Workloads -> Stateful Sets. Click the Stateful Set corresponding to your ZooKeeper deployment. In the Pods section, click zk-0 (in this example the name used in the ZooKeeper yaml is \u201czk\u201d). Click ->EXEC, this opens a new browser window. In the SHELL window, run this command: export JAVA_HOME=/opt/radiantone/rli-zookeeper-external/jdk/jre/;/opt/radiantone/rli-zookeeper-external/zookeeper/bin/zkServer.sh status Note - Use Shift+Insert to Paste. The returned value should indicate \u201cMode: follower\u201d or \u201cMode: Leader\u201d. The ZooKeeper in the following example is a follower node. Close the Browser tab and go back to the Kubernetes Dashboard. Repeat steps 3-6 for the other ZooKeeper nodes: zk-1 and zk-2. Following this example, one should be a follower node and one should be a leader node. The results are shown below. Checking the ZooKeeper Service via HTTP In the example commands used in this section, the ZooKeeper service is named \u201cmyzk\u201d and the shell is launched for the pod named myzk-2. Run the following to see the commands available: curl http://myzk:8080/commands A couple of examples are shown below. When \u201cruok\u201d returns \u201cerror\u201d : null, this means the ZooKeeper node is running fine. When \u201cis_read_only\u201d returns \u201cread_only\u201d : false, this means the ZooKeeper node is not in a read-only state. If a ZooKeeper node is in a read-only state, something is wrong and the RadiantOne nodes will not allow any write operations during this time. Most likely ZooKeeper has lost the quorum and can\u2019t communicate with more than half of the other ZooKeeper nodes. RadiantOne Nodes The StatefulSet manifests (configmap.yaml and fid.yaml) create a RadiantOne node. After the node is deployed, you can scale up the number of nodes as needed. Although the Kubernetes web console can be used to create new stateful sets, the steps below leverage the kubectl command line tool. Perform the following steps on the machine where you have downloaded the kubectl command line utilty and saved the yaml files. Installation Update the configmap.yaml file. Update the fid-aws.yaml file. Open a command prompt and navigate to the location where you installed the kubectl command line utilty. To create the RadiantOne FID StatefulSet, you can reference the folder that contains both the configmap.yaml and fid-aws.yaml files. Assuming both of these files are located in a folder located at C:\\Kubernetes\\FID, the following command can be used. kubectl --namespace=prod apply -f C:\\Kubernetes\\FID Note \u2013 if no namespace is used in your Kubernetes cluster, you can omit the --namespace property. RadiantOne Configuration The configuration can be managed from the RadiantOne Main Control Panel, kubectl command line utility, or from the Kubernetes web dashboard by launching a shell directly on the RadiantOne node/pod you want to administer. RadiantOne Main Control Panel After the RadiantOne nodes are deployed, you can view the services from the Kubernetes web dashboard and click on the link to launch the Main Control Panel. From the Kubernetes web dashboard, navigate to Discovery and Load Balancing -> Services. The external endpoints, which point to the AWS Elastic Load Balancer (ELB) that is in front of the RadiantOne services, are shown for the RadiantOne Control Panel service ( -cp). There are four external endpoints configured. Two point to the Control Panel (one is for the non-ssl port and the other is for the ssl port). Two point to the RadiantOne FID web services (SCIM, DSML/SPML, ADAP) ports which are required by the RadiantOne Main Control Panel -> Directory Browser tab. Hover over the endpoint to see the server and port. Click the link to launch the RadiantOne Main Control Panel (the one with either port 7070 for HTTP access or 7171 for HTTPS access) Login with the RadiantOne administrator. The credentials were defined in the configmap.yaml file. Manage the RadiantOne configuration as needed. For an understanding of activities that can be performed from the Main Control Panel, see the RadiantOne System Administration Guide. Kubernetes Command Line Utility The Kubernetes command line utility can be used to run commands available in RadiantOne. Below are a few examples of how to run some common commands. For details on all available commands, see the RadiantOne Command Line Configuration Guide. Example 1 - Backing up an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace: kubectl exec -it -n demo myfid-0 -- vdsconfig.sh backup-hdapstore -namingcontext o=local Example 2 - Restore an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace: kubectl exec -it -n demo myfid-0 -- vdsconfig.sh restore-hdapstore -namingcontext o=local Example 3 \u2013 Export entries from an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace. kubectl exec -it -n demo myfid-0 -- vdsconfig.sh export-ldif -basedn o=local -ldif local.ldif -scope sub The local.ldif file is created at /opt/radiantone/vds/vds_server/ldif/export/ by default. Kubernetes Web Dashboard To access the shell from the Kubernetes web dashboard: Go to Workloads -> Pods. Select the RadiantOne pod you want to manage (e.g. myfid-0) and click ->EXEC. The shell window opens. Go to the /vds folder to access the file system of RadiantOne. The example below depicts how to navigate to the log files below /vds/vds_server/logs. Enabling FIPS-Mode If your RadiantOne deployment must support FIPS 140-2 certified crypto operations for data-in-transit and data-at-rest, perform the following before enabling FIPS-mode. Modify the file /jdk/jre/lib/security/java.security and replace this line: securerandom.source=file:/dev/random With this line: securerandom.source=file:/dev/urandom Migrating Configuration RadiantOne configuration can be migrated from one environment (e.g. dev/qa) to another (e.g. production). This section describes the migration steps. Download the RadiantOne Migration Utility onto the machine where the RadiantOne configuration is to be exported. Run the following command to export the configuration. ./opt/MigrationUtility/radiantone-migration-tool-2.0.5/migrate.sh export ./opt/temp/export.zip Copy the file to the target machine where the kubectl utility is installed. Scale down the RadiantOne cluster to one node. From the Kubernetes web dashboard, go to Workloads -> Stateful Sets. Select the RadiantOne FID stateful set. Note \u2013 make sure you are in the correct Namespace! Click SCALE. Enter 1 for the (total) number of nodes the RadiantOne cluster should have. In the example shown below, there are currently 3 nodes in the RadiantOne cluster and 1 node is desired. Click OK. Kubernetes scales down to one node. Note \u2013 you must scale down instead of just stopping the RadiantOne services on the nodes. Stopping the services would result in Kubernetes trying to restart them. By default, when Kubernetes scales down the nodes, they still remain a part of the cluster. If you run cluster.sh list, you still see the nodes, but the RadiantOne services will not be running. This way, Kubernetes does not try to restart the RadiantOne services automatically. From the machine where the kubectl utility is installed, copy the file that was exported from the dev/qa environment to the RadiantOne leader node (the example below has a RadiantOne node identified as fid-0 in the demo namespace). kubectl cp export.zip fid-0:/opt/radiantone -n demo (Optional) If you want to customize additional configuration changes to be executed after the initial configuration is imported, create a script named configure_fid.sh that contains the needed commands (e.g. update host information in data sources to point to production instances). This script is executed by the migration process after the initial configuration is imported (in step #10 below). From the machine where the kubectl utility is installed, run the import command on the target pod. kubectl exec -it fid-0 -n demo -- ./migrate.sh import export.zip The migrate script stops the RadiantOne services on the target pod, and then restarts them after import. Scale up the RadiantOne nodes from the Kubernetes web dashboard -> Workloads -> Stateful Sets. Administration Scaling Out - Adding RadiantOne Nodes Scaling out means adding RadiantOne nodes to the cluster. You cannot scale out ZooKeeper nodes. The number of pods to be created is explicitly defined when the cluster is created (ZOOKEEPER_FLEET_SIZE). However, you can scale out RadiantOne nodes. To add nodes to the existing RadiantOne FID cluster: From the Kubernetes web dashboard, go to Workloads -> Stateful Sets. Select the RadiantOne FID stateful set. Note \u2013 make sure you are in the correct Namespace! Click SCALE. Enter the number (total) of nodes the RadiantOne cluster should have. In the example shown below, there are currently 2 nodes in the RadiantOne cluster and another node is going to be added. Click OK. Kubernetes adds the needed number of nodes. Once they are created, you can see the pods in Workloads -> Pods. Clients Connecting to RadiantOne FID Nodes Using LDAP The default RadiantOne configuration assumes that clients accessing RadiantOne FID are managed by the same Kubernetes cluster and will use an internal service to query RadiantOne FID as opposed to an externally facing ELB. This is dictated by the type: NodePort keyword in the .yaml file that describes the RadiantOne FID LDAP ports. Note - If you changed the .yaml file prior to deploying, and set type: LoadBalancer , then clients would access RadiantOne FID via LDAP through an Elastic Load Balancer (ELB). From the Kubernetes web console, navigate to Discovery and Load Balancing -> Services. You can see the service name and port (LDAP port 2389 by default) in the section matching the label you defined for the RadiantOne deployment. An example is shown below. Using Web Services (SCIM, DSML/SPML, REST) From the Kubernetes web dashboard, navigate to Discovery and Load Balancing -> Services. The external endpoints, which point to the AWS Elastic Load Balancer (ELB) that is in front of the RadiantOne services, are shown. There are four external endpoints configured. Two point to the Control Panel (one is for the non-ssl port and the other is for the ssl port). Two point to the RadiantOne web services (SCIM, DSML/SPML, REST/ADAP) ports. Hover over the endpoint to see the server and port. Give this information to clients to connect to RadiantOne on the web services ports. Note - You can map the ELB endpoint to a friendly DNS name by creating a Hosted Zone in AWS. Applying RadiantOne Patches Patching a version of RadiantOne is classified as an update. Patch versions are identified by the 3rd number in the whole version. For example, v7.3.10 indicates it is the 10th patch release for v7.3. You can apply a patch, and update to a newer patch release (e.g. apply a patch on v7.3.10 to update to v7.3.11). Patches are not for upgrading to a new version (e.g. moving from v7.3 to v7.4), or moving from a newer patch release to an older patch release (e.g. moving from v7.3.10 to v7.3.8). The following requirements must be met in order to update/patch the RadiantOne version on the RadiantOne nodes: The ZooKeeper ensemble must be reachable and not in a read-only state. There must be at least two nodes in the RadiantOne cluster. The process performs a rolling update where each RadiantOne node is updated at a time. While one RadiantOne node is being updated, the other node must be available to serve client requests. Although a rolling update does not require the complete RadiantOne service to be offline (at least one node should always be available), throughput will be reduced as nodes are being updated. Therefore, it is recommended that you patch during maintenance windows/non-peak hours. When a container is started, it pulls the image specified in the yaml file and Kubernetes checks the version that is installed. If the current version is less than the version outlined in the yaml file, it knows that the image needs to be updated. In this case, the update.sh file is run and one RadiantOne node is updated at a time. The steps to apply a patch are outlined below. From the Kubernetes web dashboard -> Workloads -> Stateful Sets, click the 3 dots next to the one representing the RadiantOne node. Click View/edit YAML. Update the image version in the fid-aws.yaml file to indicate a newer patch version of RadiantOne. The location to update is shown in the image below, and indicates an example of v7.3.9. Once the image is modified, the rolling update starts. This can take quite a bit of time to perform. Stateful sets are updated in order from the highest number to the lowest number. For three RadiantOne nodes, the fid-2 node is updated first, followed by the fid-1 node and finally the fid-0 node. The pod gets stopped and the latest image (indicated in the yaml file) is compared to the current version of RadiantOne on the node. If the current version is less than the version specified in the yaml, then the RadiantOne update process is executed on the node. This process is logged in the RadiantOne logs on the node. You can acces the logs from the Kubernetes web dashboard -> Workloads -> Pods -> by clicking LOGS. An example of the log is shown below. Note \u2013 A backup of the existing install is made to vds- .tar prior to updating. Deleting Deployments To remove a deployment, delete the stateful sets, services, config maps, persistent volumes, and persistent volume claims. Delete Stateful Sets From the Kubernetes Control Panel -> Overview -> Stateful Sets section, click the 3 dots next to the RadiantOne FID stateful set (in the example shown below, this is the one named fid738) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Stateful Sets section, click the 3 dots next to the ZooKeeper stateful set (in the example shown above, this is the one named zk) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the RadiantOne Control Panel service (in the example shown below, this is the one named fid738-cp) and choose Delete. Click Delete again to confirm the deletion. Delete Services From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the RadiantOne FID service (in the example shown above, this is the one named fid738) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the ZooKeeper service (in the example shown above, this is the one named zk) and choose Delete. Click Delete again to confirm the deletion. Delete Config Maps From the Kubernetes Control Panel -> Config and Storage -> Config Maps section, click the 3 dots next to the RadiantOne environment variables and choose Delete. Click Delete again to confirm the deletion. Delete Persistent Volume Claims In the Config and Storage -> Persistent Volume Claims section, click the 3 dots next to the RadiantOne persistent volume claim and choose Delete. Click Delete again to confirm the deletion. Repeat the previous step to delete the persistent volume claims for all RadiantOne nodes and all ZooKeeper nodes. Delete Persistent Volumes In the Cluster -> Persistent Volumes section, identify all of the nodes associated with the persistent volume. The value in the claim column is prefixed with the name of the namespace (e.g. demo/). Click the 3 dots next to the RadiantOne node associated with the persistent volume (it should have Status=Released) and choose Delete. Click Delete again to confirm the deletion. Repeat step 1 for all persistent volumes associated with RadiantOne and ZooKeeper nodes.","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"You can use Kubernetes to orchestrate the configuration and deployment of RadiantOne. Radiant Logic provides DevOps images for deployments on cloud platforms such as Amazon Web Services (AWS) using Amazon Elastic Kubernetes Service (EKS) and Microsoft Azure Kubernetes Service (AKS).","title":"Kubernetes"},{"location":"kubernetes/#deployment","text":"","title":"Deployment"},{"location":"kubernetes/#prerequisites","text":"Before deploying RadiantOne, you should already be familiar with Kubernetes Pods, Services and StatefulSets. See the Kubernetes Documentation for details. You should also be familiar with Kubernetes kubectl commands. See the Kubernetes Cheat Sheet for details. In addition, you should: Have a supported Kubernetes cluster running in the cloud. A commonly deployed Kubernetes cluster is Amazon Elastic Kubernetes Service . For a highly available architecture, the underlying Kubernetes cluster should support at least two pods running RadiantOne nodes and three pods running ZooKeeper. Note - Kubernetes v1.18+ is required. The pods running RadiantOne nodes need at least 2 CPUs and 4 GiB memory. The pods running ZK need at least 2 CPUs and 2 GiB memory. Install and configure the Kubernetes kubectl command-line tool on the machine where you will manage the Kubernetes cluster from. This utility controls the Kubernetes Cluster. An example is AWS CLI . Have a RadiantOne configuration exported from a Linux Dev/QA environment. After installing RadiantOne in the Kubernetes cluster, you can import the configuration and make any needed configuration updates from the RadiantOne Main Control Panel or from command line using the vdsconfig utility.","title":"Prerequisites"},{"location":"kubernetes/#manifest-files","text":"Download the StatefulSet manifests: configmap.yaml, fid-aws.yaml and zk-aws.yaml, on the machine where you will manage the Kubernetes cluster from. Contact Radiant Logic at support@radiantlogic.com for access to these files.","title":"Manifest Files"},{"location":"kubernetes/#zookeeper","text":"The StatefulSet manifest, zk-aws.yaml, creates a ZooKeeper cluster that consists of three Pods by default. Although the Kubernetes web console can be used to create new stateful sets, the steps below leverage the kubectl command line tool. Perform the following steps on the machine where you have downloaded the kubectl command line utilty and saved the yaml files.","title":"Zookeeper"},{"location":"kubernetes/#installation","text":"Update zk-aws.yaml file if necessary. You can modify the ZooKeeper ports, number of nodes (ZOOKEEPER_FLEET_SIZE) in the ensemble (3 is the default, but you can make it 5 if needed), name of the Kubernetes service linked to the ZooKeeper nodes (make sure this matches what is defined in the configmap.yaml). Note \u2013 Do not use \u201cTABS\u201d for spacing in the .yaml file. Use the space bar to indent as needed. Open a command prompt and navigate to the location where you have installed the kubectl command line tool. Create the ZooKeeper StatefulSet from the zk.yaml. Indicate the path to the ZooKeeper yaml file. In the example shown below, it is located in the same location at the kubectl tool. kubectl --namespace=prod apply -f zk-aws.yaml Note \u2013 if no namespace is used in the Kubernetes cluster, you can omit the --namespace property.","title":"Installation"},{"location":"kubernetes/#validating-the-zookeeper-service","text":"Before installing RadiantOne nodes, make sure the ZooKeeper service is running. There should be one leader node and two follower nodes (if you kept the default fleet size of 3). The steps in this section describe how to do this from the Kubernetes cluster web dashboard. From the Kubernetes web dashboard, navigate to Workloads -> Stateful Sets. Click the Stateful Set corresponding to your ZooKeeper deployment. In the Pods section, click zk-0 (in this example the name used in the ZooKeeper yaml is \u201czk\u201d). Click ->EXEC, this opens a new browser window. In the SHELL window, run this command: export JAVA_HOME=/opt/radiantone/rli-zookeeper-external/jdk/jre/;/opt/radiantone/rli-zookeeper-external/zookeeper/bin/zkServer.sh status Note - Use Shift+Insert to Paste. The returned value should indicate \u201cMode: follower\u201d or \u201cMode: Leader\u201d. The ZooKeeper in the following example is a follower node. Close the Browser tab and go back to the Kubernetes Dashboard. Repeat steps 3-6 for the other ZooKeeper nodes: zk-1 and zk-2. Following this example, one should be a follower node and one should be a leader node. The results are shown below.","title":"Validating the ZooKeeper Service"},{"location":"kubernetes/#checking-the-zookeeper-service-via-http","text":"In the example commands used in this section, the ZooKeeper service is named \u201cmyzk\u201d and the shell is launched for the pod named myzk-2. Run the following to see the commands available: curl http://myzk:8080/commands A couple of examples are shown below. When \u201cruok\u201d returns \u201cerror\u201d : null, this means the ZooKeeper node is running fine. When \u201cis_read_only\u201d returns \u201cread_only\u201d : false, this means the ZooKeeper node is not in a read-only state. If a ZooKeeper node is in a read-only state, something is wrong and the RadiantOne nodes will not allow any write operations during this time. Most likely ZooKeeper has lost the quorum and can\u2019t communicate with more than half of the other ZooKeeper nodes.","title":"Checking the ZooKeeper Service via HTTP"},{"location":"kubernetes/#radiantone-nodes","text":"The StatefulSet manifests (configmap.yaml and fid.yaml) create a RadiantOne node. After the node is deployed, you can scale up the number of nodes as needed. Although the Kubernetes web console can be used to create new stateful sets, the steps below leverage the kubectl command line tool. Perform the following steps on the machine where you have downloaded the kubectl command line utilty and saved the yaml files.","title":"RadiantOne Nodes"},{"location":"kubernetes/#installation_1","text":"Update the configmap.yaml file. Update the fid-aws.yaml file. Open a command prompt and navigate to the location where you installed the kubectl command line utilty. To create the RadiantOne FID StatefulSet, you can reference the folder that contains both the configmap.yaml and fid-aws.yaml files. Assuming both of these files are located in a folder located at C:\\Kubernetes\\FID, the following command can be used. kubectl --namespace=prod apply -f C:\\Kubernetes\\FID Note \u2013 if no namespace is used in your Kubernetes cluster, you can omit the --namespace property.","title":"Installation"},{"location":"kubernetes/#radiantone-configuration","text":"The configuration can be managed from the RadiantOne Main Control Panel, kubectl command line utility, or from the Kubernetes web dashboard by launching a shell directly on the RadiantOne node/pod you want to administer.","title":"RadiantOne Configuration"},{"location":"kubernetes/#radiantone-main-control-panel","text":"After the RadiantOne nodes are deployed, you can view the services from the Kubernetes web dashboard and click on the link to launch the Main Control Panel. From the Kubernetes web dashboard, navigate to Discovery and Load Balancing -> Services. The external endpoints, which point to the AWS Elastic Load Balancer (ELB) that is in front of the RadiantOne services, are shown for the RadiantOne Control Panel service ( -cp). There are four external endpoints configured. Two point to the Control Panel (one is for the non-ssl port and the other is for the ssl port). Two point to the RadiantOne FID web services (SCIM, DSML/SPML, ADAP) ports which are required by the RadiantOne Main Control Panel -> Directory Browser tab. Hover over the endpoint to see the server and port. Click the link to launch the RadiantOne Main Control Panel (the one with either port 7070 for HTTP access or 7171 for HTTPS access) Login with the RadiantOne administrator. The credentials were defined in the configmap.yaml file. Manage the RadiantOne configuration as needed. For an understanding of activities that can be performed from the Main Control Panel, see the RadiantOne System Administration Guide.","title":"RadiantOne Main Control Panel"},{"location":"kubernetes/#kubernetes-command-line-utility","text":"The Kubernetes command line utility can be used to run commands available in RadiantOne. Below are a few examples of how to run some common commands. For details on all available commands, see the RadiantOne Command Line Configuration Guide. Example 1 - Backing up an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace: kubectl exec -it -n demo myfid-0 -- vdsconfig.sh backup-hdapstore -namingcontext o=local Example 2 - Restore an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace: kubectl exec -it -n demo myfid-0 -- vdsconfig.sh restore-hdapstore -namingcontext o=local Example 3 \u2013 Export entries from an HDAP store mounted at o=local on the myfid-0 pod in the demo namespace. kubectl exec -it -n demo myfid-0 -- vdsconfig.sh export-ldif -basedn o=local -ldif local.ldif -scope sub The local.ldif file is created at /opt/radiantone/vds/vds_server/ldif/export/ by default.","title":"Kubernetes Command Line Utility"},{"location":"kubernetes/#kubernetes-web-dashboard","text":"To access the shell from the Kubernetes web dashboard: Go to Workloads -> Pods. Select the RadiantOne pod you want to manage (e.g. myfid-0) and click ->EXEC. The shell window opens. Go to the /vds folder to access the file system of RadiantOne. The example below depicts how to navigate to the log files below /vds/vds_server/logs.","title":"Kubernetes Web Dashboard"},{"location":"kubernetes/#enabling-fips-mode","text":"If your RadiantOne deployment must support FIPS 140-2 certified crypto operations for data-in-transit and data-at-rest, perform the following before enabling FIPS-mode. Modify the file /jdk/jre/lib/security/java.security and replace this line: securerandom.source=file:/dev/random With this line: securerandom.source=file:/dev/urandom","title":"Enabling FIPS-Mode"},{"location":"kubernetes/#migrating-configuration","text":"RadiantOne configuration can be migrated from one environment (e.g. dev/qa) to another (e.g. production). This section describes the migration steps. Download the RadiantOne Migration Utility onto the machine where the RadiantOne configuration is to be exported. Run the following command to export the configuration. ./opt/MigrationUtility/radiantone-migration-tool-2.0.5/migrate.sh export ./opt/temp/export.zip Copy the file to the target machine where the kubectl utility is installed. Scale down the RadiantOne cluster to one node. From the Kubernetes web dashboard, go to Workloads -> Stateful Sets. Select the RadiantOne FID stateful set. Note \u2013 make sure you are in the correct Namespace! Click SCALE. Enter 1 for the (total) number of nodes the RadiantOne cluster should have. In the example shown below, there are currently 3 nodes in the RadiantOne cluster and 1 node is desired. Click OK. Kubernetes scales down to one node. Note \u2013 you must scale down instead of just stopping the RadiantOne services on the nodes. Stopping the services would result in Kubernetes trying to restart them. By default, when Kubernetes scales down the nodes, they still remain a part of the cluster. If you run cluster.sh list, you still see the nodes, but the RadiantOne services will not be running. This way, Kubernetes does not try to restart the RadiantOne services automatically. From the machine where the kubectl utility is installed, copy the file that was exported from the dev/qa environment to the RadiantOne leader node (the example below has a RadiantOne node identified as fid-0 in the demo namespace). kubectl cp export.zip fid-0:/opt/radiantone -n demo (Optional) If you want to customize additional configuration changes to be executed after the initial configuration is imported, create a script named configure_fid.sh that contains the needed commands (e.g. update host information in data sources to point to production instances). This script is executed by the migration process after the initial configuration is imported (in step #10 below). From the machine where the kubectl utility is installed, run the import command on the target pod. kubectl exec -it fid-0 -n demo -- ./migrate.sh import export.zip The migrate script stops the RadiantOne services on the target pod, and then restarts them after import. Scale up the RadiantOne nodes from the Kubernetes web dashboard -> Workloads -> Stateful Sets.","title":"Migrating Configuration"},{"location":"kubernetes/#administration","text":"","title":"Administration"},{"location":"kubernetes/#scaling-out-adding-radiantone-nodes","text":"Scaling out means adding RadiantOne nodes to the cluster. You cannot scale out ZooKeeper nodes. The number of pods to be created is explicitly defined when the cluster is created (ZOOKEEPER_FLEET_SIZE). However, you can scale out RadiantOne nodes. To add nodes to the existing RadiantOne FID cluster: From the Kubernetes web dashboard, go to Workloads -> Stateful Sets. Select the RadiantOne FID stateful set. Note \u2013 make sure you are in the correct Namespace! Click SCALE. Enter the number (total) of nodes the RadiantOne cluster should have. In the example shown below, there are currently 2 nodes in the RadiantOne cluster and another node is going to be added. Click OK. Kubernetes adds the needed number of nodes. Once they are created, you can see the pods in Workloads -> Pods.","title":"Scaling Out - Adding RadiantOne Nodes"},{"location":"kubernetes/#clients-connecting-to-radiantone-fid-nodes","text":"","title":"Clients Connecting to RadiantOne FID Nodes"},{"location":"kubernetes/#using-ldap","text":"The default RadiantOne configuration assumes that clients accessing RadiantOne FID are managed by the same Kubernetes cluster and will use an internal service to query RadiantOne FID as opposed to an externally facing ELB. This is dictated by the type: NodePort keyword in the .yaml file that describes the RadiantOne FID LDAP ports. Note - If you changed the .yaml file prior to deploying, and set type: LoadBalancer , then clients would access RadiantOne FID via LDAP through an Elastic Load Balancer (ELB). From the Kubernetes web console, navigate to Discovery and Load Balancing -> Services. You can see the service name and port (LDAP port 2389 by default) in the section matching the label you defined for the RadiantOne deployment. An example is shown below.","title":"Using LDAP"},{"location":"kubernetes/#using-web-services-scim-dsmlspml-rest","text":"From the Kubernetes web dashboard, navigate to Discovery and Load Balancing -> Services. The external endpoints, which point to the AWS Elastic Load Balancer (ELB) that is in front of the RadiantOne services, are shown. There are four external endpoints configured. Two point to the Control Panel (one is for the non-ssl port and the other is for the ssl port). Two point to the RadiantOne web services (SCIM, DSML/SPML, REST/ADAP) ports. Hover over the endpoint to see the server and port. Give this information to clients to connect to RadiantOne on the web services ports. Note - You can map the ELB endpoint to a friendly DNS name by creating a Hosted Zone in AWS.","title":"Using Web Services (SCIM, DSML/SPML, REST)"},{"location":"kubernetes/#applying-radiantone-patches","text":"Patching a version of RadiantOne is classified as an update. Patch versions are identified by the 3rd number in the whole version. For example, v7.3.10 indicates it is the 10th patch release for v7.3. You can apply a patch, and update to a newer patch release (e.g. apply a patch on v7.3.10 to update to v7.3.11). Patches are not for upgrading to a new version (e.g. moving from v7.3 to v7.4), or moving from a newer patch release to an older patch release (e.g. moving from v7.3.10 to v7.3.8). The following requirements must be met in order to update/patch the RadiantOne version on the RadiantOne nodes: The ZooKeeper ensemble must be reachable and not in a read-only state. There must be at least two nodes in the RadiantOne cluster. The process performs a rolling update where each RadiantOne node is updated at a time. While one RadiantOne node is being updated, the other node must be available to serve client requests. Although a rolling update does not require the complete RadiantOne service to be offline (at least one node should always be available), throughput will be reduced as nodes are being updated. Therefore, it is recommended that you patch during maintenance windows/non-peak hours. When a container is started, it pulls the image specified in the yaml file and Kubernetes checks the version that is installed. If the current version is less than the version outlined in the yaml file, it knows that the image needs to be updated. In this case, the update.sh file is run and one RadiantOne node is updated at a time. The steps to apply a patch are outlined below. From the Kubernetes web dashboard -> Workloads -> Stateful Sets, click the 3 dots next to the one representing the RadiantOne node. Click View/edit YAML. Update the image version in the fid-aws.yaml file to indicate a newer patch version of RadiantOne. The location to update is shown in the image below, and indicates an example of v7.3.9. Once the image is modified, the rolling update starts. This can take quite a bit of time to perform. Stateful sets are updated in order from the highest number to the lowest number. For three RadiantOne nodes, the fid-2 node is updated first, followed by the fid-1 node and finally the fid-0 node. The pod gets stopped and the latest image (indicated in the yaml file) is compared to the current version of RadiantOne on the node. If the current version is less than the version specified in the yaml, then the RadiantOne update process is executed on the node. This process is logged in the RadiantOne logs on the node. You can acces the logs from the Kubernetes web dashboard -> Workloads -> Pods -> by clicking LOGS. An example of the log is shown below. Note \u2013 A backup of the existing install is made to vds- .tar prior to updating.","title":"Applying RadiantOne Patches"},{"location":"kubernetes/#deleting-deployments","text":"To remove a deployment, delete the stateful sets, services, config maps, persistent volumes, and persistent volume claims.","title":"Deleting Deployments"},{"location":"kubernetes/#delete-stateful-sets","text":"From the Kubernetes Control Panel -> Overview -> Stateful Sets section, click the 3 dots next to the RadiantOne FID stateful set (in the example shown below, this is the one named fid738) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Stateful Sets section, click the 3 dots next to the ZooKeeper stateful set (in the example shown above, this is the one named zk) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the RadiantOne Control Panel service (in the example shown below, this is the one named fid738-cp) and choose Delete. Click Delete again to confirm the deletion.","title":"Delete Stateful Sets"},{"location":"kubernetes/#delete-services","text":"From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the RadiantOne FID service (in the example shown above, this is the one named fid738) and choose Delete. Click Delete again to confirm the deletion. From the Kubernetes Control Panel -> Overview -> Services section, click the 3 dots next to the ZooKeeper service (in the example shown above, this is the one named zk) and choose Delete. Click Delete again to confirm the deletion.","title":"Delete Services"},{"location":"kubernetes/#delete-config-maps","text":"From the Kubernetes Control Panel -> Config and Storage -> Config Maps section, click the 3 dots next to the RadiantOne environment variables and choose Delete. Click Delete again to confirm the deletion.","title":"Delete Config Maps"},{"location":"kubernetes/#delete-persistent-volume-claims","text":"In the Config and Storage -> Persistent Volume Claims section, click the 3 dots next to the RadiantOne persistent volume claim and choose Delete. Click Delete again to confirm the deletion. Repeat the previous step to delete the persistent volume claims for all RadiantOne nodes and all ZooKeeper nodes.","title":"Delete Persistent Volume Claims"},{"location":"kubernetes/#delete-persistent-volumes","text":"In the Cluster -> Persistent Volumes section, identify all of the nodes associated with the persistent volume. The value in the claim column is prefixed with the name of the namespace (e.g. demo/). Click the 3 dots next to the RadiantOne node associated with the persistent volume (it should have Status=Released) and choose Delete. Click Delete again to confirm the deletion. Repeat step 1 for all persistent volumes associated with RadiantOne and ZooKeeper nodes.","title":"Delete Persistent Volumes"},{"location":"postmigrationscript/","text":"Post Migration Script Additional configuration commands can be listed in a script file named configure_fid.sh. The script should contain one command per line in the file and be located in a folder named scripts (e.g. /opt/radiantone/scripts) on the RadiantOne target pod. Any command available in the vdsconfig utility can be included in the configure_fid.sh script. For information on the vdsconfig commands, see the RadiantOne Command Line Configuration guide. The migrate script checks for the configure_fid.sh script and executes it after the the initial configuration is imported. Create Script On the machine where the kubectl utility is installed, create a text file named configure_fid.sh. In an editor, edit configure_fid.sh and add the commands that should be executed after the configuration is imported into the RadiantOne production pod. There should be one command per line in the script file. Some common commands are ones that update data sources to point to production instances. An example of a script containing commands to update two data sources (named addomain1, and addomain2) to point to production instances is shown below. Copy Script On the RadiantOne leader pod (where the configuration is going to be imported), create a folder named scripts in /opt/radiantone. On the machine where the kubectl utility is installed, and the configure_fid.sh script is located, copy the configure_fid.sh script file to the target RadiantOne pod (the example below has a RadiantOne node identified as fid-0 in the demo namespace). kubectl cp configure_fid.sh fid-0:/opt/radiantone/scripts -n demo On the machine where the kubectl utility is installed, run the migrate.sh script to import the configuration into the RadiantOne production pod. This script imports the configuration and then runs the configure_fid.sh script to execute the additional configuration commands. The example below has a RadiantOne node identified as fid-0 in the demo namespace. kubectl exec -it fid-0 -n demo -- ./migrate.sh import export.zip","title":"Post Migration Script"},{"location":"postmigrationscript/#post-migration-script","text":"Additional configuration commands can be listed in a script file named configure_fid.sh. The script should contain one command per line in the file and be located in a folder named scripts (e.g. /opt/radiantone/scripts) on the RadiantOne target pod. Any command available in the vdsconfig utility can be included in the configure_fid.sh script. For information on the vdsconfig commands, see the RadiantOne Command Line Configuration guide. The migrate script checks for the configure_fid.sh script and executes it after the the initial configuration is imported.","title":"Post Migration Script"},{"location":"postmigrationscript/#create-script","text":"On the machine where the kubectl utility is installed, create a text file named configure_fid.sh. In an editor, edit configure_fid.sh and add the commands that should be executed after the configuration is imported into the RadiantOne production pod. There should be one command per line in the script file. Some common commands are ones that update data sources to point to production instances. An example of a script containing commands to update two data sources (named addomain1, and addomain2) to point to production instances is shown below.","title":"Create Script"},{"location":"postmigrationscript/#copy-script","text":"On the RadiantOne leader pod (where the configuration is going to be imported), create a folder named scripts in /opt/radiantone. On the machine where the kubectl utility is installed, and the configure_fid.sh script is located, copy the configure_fid.sh script file to the target RadiantOne pod (the example below has a RadiantOne node identified as fid-0 in the demo namespace). kubectl cp configure_fid.sh fid-0:/opt/radiantone/scripts -n demo On the machine where the kubectl utility is installed, run the migrate.sh script to import the configuration into the RadiantOne production pod. This script imports the configuration and then runs the configure_fid.sh script to execute the additional configuration commands. The example below has a RadiantOne node identified as fid-0 in the demo namespace. kubectl exec -it fid-0 -n demo -- ./migrate.sh import export.zip","title":"Copy Script"},{"location":"terraform/","text":"FID on EKS using Terraform EKS Amazon Elastic Kubernetes service is a managed Kubernetes service provided by AWS. EKS is a certified Kubernetes Conformant, so existing applications that run in Kubernetes are compatible with EKS. Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes and also provides advantages like performance, scaling, reliability and, availability Integration with other AWS services like networking, security, IAM and, VPC is also supported making EKS suitable for cloud applications and services. Learn more about EKS here Terraform Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Terraform must be installed, refer to documentation for more details. Prerequisites Terraform (terraform CLI) AWS CLI configured Machine with kubectl installed Valid FID License (if you do not have a valid license please contact support@radiantlogic.com ) Helm installed (optional) Deployment Terraform connects to AWS using the configuration details provided (either through a tfvars file or configured in the cli where terraform commands will be run) to deploy the necessary resources in a highly configurable manner Components that will be created and deployed VPC Subnets Security Groups Worker Nodes NAT Gateway EKS Cluster Kubernetes FID using Helm Charts NOTE: Navigate to the location where you have either cloned or downloaded the terraform files. Terraform INIT The terraform init command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times. Initialize terraform terraform init [options] Terraform VALIDATE The terraform validate command validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc. Validate the terraform configuration and existing file terraform validate [options] Terraform PLAN The terraform plan command creates an execution plan. By default, creating a plan consists of: Reading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration. Create the execution file terraform plan [options] Terraform APPLY The terraform apply command executes the actions proposed in a Terraform plan. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to approve that plan, before taking the indicated actions. Another way to use terraform apply is to pass it the filename of a saved plan file you created earlier with terraform plan -out=..., in which case Terraform will apply the changes in the plan without any confirmation prompt. This two-step workflow is primarily intended for when running Terraform in automation Apply the execution (plan file) file created terraform apply [options] [plan file] Terraform DESTROY The terraform destroy command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. While you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work. terraform destroy [options] REQUIREMENTS Name Version terraform >=0.13.1 aws >=3.63 PROVIDERS Name Version aws >=3.63 helm INPUTS Name Description region AWS region namespace Name of the namespace that will be created in kubernetes environment cluster_name Name of the EKS cluster. Also used as a prefix in names of related resources cluster_version Kubernetes version to use for the EKS cluster workers_group_defaults Override default values for target groups. See workers_group_defaults_defaults in local.tf for valid keys subnets A list of subnets to place the EKS cluster and workers within worker_groups A list of maps defining worker group configurations to be defined using AWS Launch Configurations. See workers_group_defaults for valid keys instance_type The type of instance to start create_spot_instance Depicts if the instance is a spot instance create_vpc Controls if VPC should be created (it affects almost all resources) vpc_name Name to be used on all the resources as identifier cidr The CIDR block for the VPC. Default value is a valid CIDR, but not acceptable by AWS and should be overridden public_subnets A list of public subnets inside the VPC private_subnets A list of private subnets inside the VPC enable_nat_gateway Should be true if you want to provision NAT Gateways for each of your private networks single_nat_gatewa Should be true if you want to provision a single shared NAT Gateway across all of your private networks enable_dns_support Should be true to enable DNS hostnames in the VPC one_nat_gateway_per_az Should be true to enable only one NAT gateway per availability zone public_subnet_tags Additional tags for the public subnets private_subnet_tags Additional tags for the private subnets application_name Name of the application application_name2 OUTPUTS Name Description region AWS region cluster_id EKS cluster ID cluster_endpoint Endpoint for EKS control plane cluster_security_group_id Security group ids attached to the cluster control plane kubectl_config kubectl config as generated by the module config_map_aws_auth A kubernetes configuration to authenticate to this EKS cluster. cluster_name Kubernetes Cluster Name vpc_resource_level_tags tags vpc_all_tags tags","title":"Terraform"},{"location":"terraform/#fid-on-eks-using-terraform","text":"","title":"FID on EKS using Terraform"},{"location":"terraform/#eks","text":"Amazon Elastic Kubernetes service is a managed Kubernetes service provided by AWS. EKS is a certified Kubernetes Conformant, so existing applications that run in Kubernetes are compatible with EKS. Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes and also provides advantages like performance, scaling, reliability and, availability Integration with other AWS services like networking, security, IAM and, VPC is also supported making EKS suitable for cloud applications and services. Learn more about EKS here","title":"EKS"},{"location":"terraform/#terraform","text":"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Terraform must be installed, refer to documentation for more details.","title":"Terraform"},{"location":"terraform/#prerequisites","text":"Terraform (terraform CLI) AWS CLI configured Machine with kubectl installed Valid FID License (if you do not have a valid license please contact support@radiantlogic.com ) Helm installed (optional)","title":"Prerequisites"},{"location":"terraform/#deployment","text":"Terraform connects to AWS using the configuration details provided (either through a tfvars file or configured in the cli where terraform commands will be run) to deploy the necessary resources in a highly configurable manner","title":"Deployment"},{"location":"terraform/#components-that-will-be-created-and-deployed","text":"VPC Subnets Security Groups Worker Nodes NAT Gateway EKS Cluster Kubernetes FID using Helm Charts","title":"Components that will be created and deployed"},{"location":"terraform/#note-navigate-to-the-location-where-you-have-either-cloned-or-downloaded-the-terraform-files","text":"","title":"NOTE: Navigate to the location where you have either cloned or downloaded the terraform files."},{"location":"terraform/#terraform-init","text":"The terraform init command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times. Initialize terraform terraform init [options]","title":"Terraform INIT"},{"location":"terraform/#terraform-validate","text":"The terraform validate command validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc. Validate the terraform configuration and existing file terraform validate [options]","title":"Terraform VALIDATE"},{"location":"terraform/#terraform-plan","text":"The terraform plan command creates an execution plan. By default, creating a plan consists of: Reading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration. Create the execution file terraform plan [options]","title":"Terraform PLAN"},{"location":"terraform/#terraform-apply","text":"The terraform apply command executes the actions proposed in a Terraform plan. The most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to approve that plan, before taking the indicated actions. Another way to use terraform apply is to pass it the filename of a saved plan file you created earlier with terraform plan -out=..., in which case Terraform will apply the changes in the plan without any confirmation prompt. This two-step workflow is primarily intended for when running Terraform in automation Apply the execution (plan file) file created terraform apply [options] [plan file]","title":"Terraform APPLY"},{"location":"terraform/#terraform-destroy","text":"The terraform destroy command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. While you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work. terraform destroy [options]","title":"Terraform DESTROY"},{"location":"terraform/#requirements","text":"Name Version terraform >=0.13.1 aws >=3.63","title":"REQUIREMENTS"},{"location":"terraform/#providers","text":"Name Version aws >=3.63 helm","title":"PROVIDERS"},{"location":"terraform/#inputs","text":"Name Description region AWS region namespace Name of the namespace that will be created in kubernetes environment cluster_name Name of the EKS cluster. Also used as a prefix in names of related resources cluster_version Kubernetes version to use for the EKS cluster workers_group_defaults Override default values for target groups. See workers_group_defaults_defaults in local.tf for valid keys subnets A list of subnets to place the EKS cluster and workers within worker_groups A list of maps defining worker group configurations to be defined using AWS Launch Configurations. See workers_group_defaults for valid keys instance_type The type of instance to start create_spot_instance Depicts if the instance is a spot instance create_vpc Controls if VPC should be created (it affects almost all resources) vpc_name Name to be used on all the resources as identifier cidr The CIDR block for the VPC. Default value is a valid CIDR, but not acceptable by AWS and should be overridden public_subnets A list of public subnets inside the VPC private_subnets A list of private subnets inside the VPC enable_nat_gateway Should be true if you want to provision NAT Gateways for each of your private networks single_nat_gatewa Should be true if you want to provision a single shared NAT Gateway across all of your private networks enable_dns_support Should be true to enable DNS hostnames in the VPC one_nat_gateway_per_az Should be true to enable only one NAT gateway per availability zone public_subnet_tags Additional tags for the public subnets private_subnet_tags Additional tags for the private subnets application_name Name of the application application_name2","title":"INPUTS"},{"location":"terraform/#outputs","text":"Name Description region AWS region cluster_id EKS cluster ID cluster_endpoint Endpoint for EKS control plane cluster_security_group_id Security group ids attached to the cluster control plane kubectl_config kubectl config as generated by the module config_map_aws_auth A kubernetes configuration to authenticate to this EKS cluster. cluster_name Kubernetes Cluster Name vpc_resource_level_tags tags vpc_all_tags tags","title":"OUTPUTS"}]}